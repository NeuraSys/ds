{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Błąd obciążenia i wariancji - kompromis\n",
    "\n",
    "Autor sekcji: {ref}`authors:filip-wojcik`\n",
    "\n",
    "Niniejszy podrozdział przedstawia kolejny, bardzo istotny aspekt uczenia maszynowego, jakim jest kompromis pomiędzy błędem obciążenia i wariancji. Jest to jedna z najczęściej przywoływanych zależności w uczeniu maszynowym, która ma kluczowe znaczenie w procesie budowy modeli predykcyjnych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kompromis między obciążeniem a wariancją\n",
    "\n",
    "W kontekście uczenia maszynowego, mówimy o **kompromisie między obciążeniem a wariancją** (ang. *bias-variance tradeoff*). Jest to jedno z kluczowych pojęć, które pozwala zrozumieć, jakie są dwa najczęściej pojawiające się błędy, popełniane przez modele.\n",
    "\n",
    "Zacznijmy jednak od obrazu, który mówi więcej niż tysiąc słów. W kontekście kompromisu między obciążeniem a wariancją bardzo często przywołuje się analogię z tarczami strzelniczymi - taką, jak pokazana poniżej.\n",
    "\n",
    "\n",
    "```{figure} ../images/bias-variance.png\n",
    "---\n",
    "width: 900px\n",
    "name: bias-variance\n",
    "---\n",
    "Obciążenie i wariancja - targe strzelnicze. Źródło: [Devopedia - bias variance tradeoff](https://devopedia.org/bias-variance-trade-off)\n",
    "```\n",
    "\n",
    "Na rysunku widzimy cztery hipotetyczne sytuacje:\n",
    "\n",
    "1. **Low bias + low variance / niskie obciążenie + niska wariancja** - wszystkie trafienia układają się blisko centrum i nie są zbyt rozproszone. Oznacza to, że model jest w stanie dobrze przewidywać wartości, a jego trafienia są stabilne.\n",
    "2. **Low bias + high variance / niskie obciążenie + wysoka wariancja** - trafienia są rozproszone, ale układają się blisko centrum. Oznacza to, że model jest w stanie średnio i co do zasady dobrze przewidywać wyniki, ale jego trafienia są niestabilne.\n",
    "3. **High bias + low variance / wysokie obciążenie + niska wariancja** - trafienia są skupione, ale daleko od centrum. Oznacza to, że model ma problem z trafnym przewidywaniem w sposób systematyczny, ale jego trafienia są stabilne.\n",
    "4. **High bias + high variance / wysokie obciążenie + wysoka wariancja** - najgorsza możliwa sytuacja. Trafienia są rozproszone i daleko od centrum. Oznacza to, że model ma problem z trafnym przewidywaniem w sposób systematyczny, a jego trafienia są niestabilne.\n",
    "\n",
    "Każdy strzał do tarczy strzelniczej możemy traktować jako pojedyncze przewidywanie modelu. W praktyce, chcemy, aby nasz model był w stanie przewidywać wyniki w sposób **stabilny** i **trafny**, co niestety nie zawsze jest możliwe.\n",
    "\n",
    "```{admonition} Czynniki sprzyjające błędowi obciążenia i wariancji\n",
    ":class: tip\n",
    "\n",
    "W praktyce obserwujemy często następującą zależność:\n",
    "\n",
    "1. **Im większa złożoność modelu - tym większa wariancja i mniejsze obciążenie**. Wynika to z faktu, że bardziej skomplikowany model (np. sieć neuronowa) jest w stanie reprezentować niezwykle złożone zależności w danych, ale jednocześnie jest bardziej podatny na zmiany w danych uczących i może to doprowadzić do niestabilnych predykcji.\n",
    "2. **Im mniejsza złożoność modelu - tym mniejsza wariancja i większe obciążenie**. Prostsze modele (np. regresja liniowa) są mniej podatne na zmiany w danych uczących, ale jednocześnie mają ograniczoną zdolność do reprezentowania złożonych zależności w danych.\n",
    "```\n",
    "\n",
    "Mając na uwadze powyższe elementy, możemy teraz zdefiniować **obciążenie**, **wariancję** i **kompromis między nimi**.\n",
    "\n",
    "```{glossary}\n",
    "Błąd obciążenia modelu / Prediction bias\n",
    "    Obciążenie modelu to systematyczny błąd, który wynika z uproszczeń, jakie wprowadzamy w procesie uczenia. W praktyce oznacza to, że jego predykcja mijają się z wartością docelową. {cite:ps}`abu2012learning` {cite:ps}`domingos2012few`\n",
    "\n",
    "Błąd wariancji modelu / Prediction variance\n",
    "    Wariancja modelu to miara tego, jak bardzo różnią się predykcje modelu, w zależności od zmian w zbiorze uczącym. Wysoka wariancja oznacza, że model często zmienia swoje decyzje, w zależności od niewielkich wahań w danych. {cite:ps}`abu2012learning` {cite:ps}`domingos2012few`.\n",
    "\n",
    "Kompromis między obciążeniem a wariancją / Bias-variance tradeoff\n",
    "    Kompromis między obciążeniem a wariancją to pojęcie, które mówi o tym, że w praktyce gdy zwiększamy złożoność modelu - redukując jego błąd obciążenia, zwiększamy jednocześnie wariancję. Zmniejszając złożoność modelu - zwiększamy błąd obciążenia, ale jednocześnie zmniejszamy wariancję. Bardzo rzadko możliwe jest jednoczesne zmniejszenie obydwu rodzajów błędu. {cite:ps}`abu2012learning` {cite:ps}`domingos2012few` {cite:ps}`sammut2017encyclopedia`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zanim przejdziemy do praktycznych przykładów, zdefiniujemy powyższe elementy w postaci formalnej.\n",
    "\n",
    "Mając dany konkretny zestaw danych $\\mathbf{X}_i$ lub skrótowo $\\mathbf{X}$,oraz docelowych etykiet/kategorii (czyli nasze {term}`Pojęcie docelowe`, którego szukamy) $\\mathbf{y} = f(\\mathbf{X})$, zbiór wielu takich zestawów: $\\mathcal{D}=\\{(\\mathbf{X}_1, \\mathbf{y}_1),  (\\mathbf{X}_2,, \\mathbf{y}_2), \\dots, (\\mathbf{X}_k, \\mathbf{y}_k) \\}$, model uczenia maszynowego $h$, dokonujący predykcji na tym zbiorze $\\hat{y} = h(\\mathbf{X})$, możemy zdefiniować błąd modelu jako sumę błędu obciążenia, błędu wariancji i błędu nieredukowalnego w kontekście.\n",
    "\n",
    "Zaczynamy od definicji naszej funkcji kosztu - posłużymy się błędem średniokwadratowym:\n",
    "\n",
    "```{math}\n",
    "S = (y - \\hat{y})^2\n",
    "```\n",
    "\n",
    "Posługując się wzorem skróconego mnożenia $(a+b)^2 = a^2 + 2ab + b^2$ oraz przekształceniem algebraicznym polegającym na dodaniu i odjęciu tej samej wartości - w tym przypadku wartości oczekiwanej predykcji (czyli średniej predykcji) dla wszystkich zbiorów danych, możemy zapisać:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "S &= (y - \\hat{y})^2  \\\\\n",
    "&= (y - E_{\\mathcal{D}}[\\hat{y}] + E_{\\mathcal{D}}[\\hat{y}] - \\hat{y})^2 \\\\\n",
    "&= (y - E_{\\mathcal{D}}[\\hat{y}])^2 + 2(y - E_{\\mathcal{D}}[\\hat{y}])(E_{\\mathcal{D}}[\\hat{y}] - \\hat{y}) + (E_{\\mathcal{D}}[\\hat{y}] - \\hat{y})^2  \\\\\n",
    "\\end{aligned}\n",
    "$$ (bias-variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W równaniu {eq}`bias-variance` powyżej traktujemy człony\n",
    "1. $y - E_{\\mathcal{D}}[\\hat{y}]$ jako *a* we wzorze skróconego mnożenia\n",
    "2. $E_{\\mathcal{D}}[\\hat{y}] - \\hat{y}$ jako *b* we wzorze skróconego mnożenia\n",
    "i potem korzystamy z własności wzoru skróconego mnożenia.\n",
    "\n",
    "Ponieważ uczenie modelu jest procesem stochastycznym, musimy uśrednić ten wynik dla wielu różnych zbiorów danych. Do tego służy operator **wartości oczekiwanej** $E_{\\mathcal{D}}[\\cdot]$. Zanim to zrobimy, zwróćmy uwagę, że:\n",
    "\n",
    "1. $y$ nie jest zależne od zbioru danych - to deterministyczna funkcja, {term}`Pojęcie docelowe`, które chcemy przewidzieć - ono jest raz na zawsze ustalone. Tym samym $E_{\\mathcal{D}}[y] = y$\n",
    "2. $E_{\\mathcal{D}}[\\hat{y}]$ to średnia predykcja modelu na wielu różnych zbiorach danych. To jest nasze **obciążenie** - czyli błąd, który wynika z uproszczeń, jakie wprowadzamy w procesie uczenia. Ta część nie będzie stałą, ale będzie silnie zależeć od specyfiki zbioru danych.\n",
    "3. Z ogólnej własności operatora własności oczekiwanej wiemy, że: $E[E[x]] = E[x]$.\n",
    "4. Z definicji wariancji, wiemy, że jest ona definiowana jako: $Var(x) = E[(x - E[x])^2]$\n",
    "\n",
    "Zobaczmy teraz, jak możemy zdefiniować błąd obciążenia i wariancji w kontekście powyższego równania.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_{\\mathcal{D}}[S] &= E_{\\mathcal{D}}[(y - \\hat{y})^2 ]  \\\\\n",
    "&= E_\\mathcal{D}\\left[(y - E_{\\mathcal{D}}[\\hat{y}])^2 + 2(y - E_{\\mathcal{D}}[\\hat{y}])(E_{\\mathcal{D}}[\\hat{y}] - \\hat{y}) + (E_{\\mathcal{D}}[\\hat{y}] - \\hat{y})^2\\right]  \\\\\n",
    "&= E_{\\mathcal{D}}[(y - E_{\\mathcal{D}}[\\hat{y}])^2] + E_{\\mathcal{D}}[2(y - E_{\\mathcal{D}}[\\hat{y}])(E_{\\mathcal{D}}[\\hat{y}] - \\hat{y})] + E_{\\mathcal{D}}[(E_{\\mathcal{D}}[\\hat{y}] - \\hat{y})^2]  \\\\\n",
    "&= (y - E_{\\mathcal{D}}[\\hat{y}])^2 + E_{\\mathcal{D}}[(E_{\\mathcal{D}}[\\hat{y}] - \\hat{y})^2]  \\\\\n",
    "&= [\\text{błąd obciążenia}^2] + \\text{wariancja}\n",
    "\\end{aligned}\n",
    "$$(bias-variance-derivation)\n",
    "\n",
    "W równaniu {eq}`bias-variance-derivation` dużo się dzieje, ale (wbrew pozorom) są to dość podstawowe przekształcenia, wynikające z definicji podstawowych terminów probabilistyki i statystyki.\n",
    "\n",
    "* **Bias** (czyli nasz **bład obciążenia**) uzyskuje definicję: \n",
    "\n",
    "$$\\text{błąd obciążenia} = (y - E_{\\mathcal{D}}[\\hat{y}])^2$$\n",
    "\n",
    ", możemy zatem o nim myśleć, jako o **średnim błędzie kwadratowym pomiędzy pojeciem docelowym, a wartością oczekiwaną (np. średnią) predykcji różnych modeli dla tego samego zbioru danych**.\n",
    "* **Wariancja** uzyskuje definicję \n",
    "   \n",
    "   $$\\text{wariancja} = E_{\\mathcal{D}}[(E_{\\mathcal{D}}[\\hat{y}] - \\hat{y})^2]$$\n",
    "\n",
    ", możemy zatem o niej myśleć, jako o **średnim błędzie kwadratowym pomiędzy średnią predykcją różnych modeli dla tego samego zbioru danych, a predykcją konkretnego modelu**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} Znikający wyraz\n",
    ":class: tip\n",
    "Uważna Czytelniczka lub Czytelnik stwierdzi, że w {eq}`bias-variance-derivation` w przedostatniej linijce znika wyraz $E_{\\mathcal{D}}[2(y - E_{\\mathcal{D}}[\\hat{y}])(E_{\\mathcal{D}}[\\hat{y}] - \\hat{y})].$\n",
    "Dlaczego tak się dzieje?\n",
    "\n",
    "Otóż po zastosowaniu operatora wartości oczekiwanej, ten wyraz upraszcza się i skraca do zera. Warto to sprawdzić samemu (słynne: \"pozostawiamy to jako ćwiczenie...\"). Zachęcam do sprawdzenia samodzielnie tej zależności. W razie czego, poniżej znajduje się jej wyprowadzenie.\n",
    "\n",
    "```{dropdown} Wyprowadzenie odpowiedzi\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_{\\mathcal{D}}[2(y - E_{\\mathcal{D}}[\\hat{y}])(E_{\\mathcal{D}}[\\hat{y}] - \\hat{y})] &= 2E_{\\mathcal{D}}[(y - E_{\\mathcal{D}}[\\hat{y}])(E_{\\mathcal{D}}[\\hat{y}] - \\hat{y})]  \\\\\n",
    "&= 2(y - E_{\\mathcal{D}}[\\hat{y}])E_{\\mathcal{D}}[E_{\\mathcal{D}}[\\hat{y}] - \\hat{y}]  \\\\\n",
    "&= 2(y - E_{\\mathcal{D}}[\\hat{y}])E_{\\mathcal{D}}[E_{\\mathcal{D}}[\\hat{y}] - E_{\\mathcal{D}}[\\hat{y}]]  \\\\\n",
    "&= 2(y - E_{\\mathcal{D}}[\\hat{y}])E_{\\mathcal{D}}[0]  \\\\\n",
    "&= 0.\n",
    "\\end{aligned}\n",
    "$$(bias-variance)\n",
    "\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praktyczne zastosowanie\n",
    "\n",
    "Czy da się to jakoś zastosować w praktyce, albo zbadać za pomocą narzędzi? Owszem. W praktyce, możemy zastosować **walidację krzyżową** (ang. *cross-validation*), aby zbadać, jak zmieniają się błąd obciążenia i wariancja w zależności od złożoności modelu.\n",
    "\n",
    "Ponadto, mamy gotowe biblioteki, które pomagają nam wyznaczać obciążenie i wariancję poszczególnych modeli dla danego zbioru danych / problemu. w ten sposób, możemy wybrać alternatywę, która stanowi rozsądny kompromis.\n",
    "\n",
    "### Przykład\n",
    "\n",
    "W przykładzie poniżej zbadamy rozkład obciążenia i wariancji kilku modeli dla zbioru danych [California housing](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html) - zawierającego informacje o cenach nieruchomości w Kalifornii, w zależności od wielu różnych czynników."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  \n",
       "1    -122.22  \n",
       "2    -122.24  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "from time import time\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "california = fetch_california_housing()\n",
    "X = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "y = california.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nasz zbiór danych składa się z 20640 obserwacji i 8 zmiennych objaśniających. Naszym celem jest przewidzenie ceny nieruchomości na podstawie tych zmiennych.\n",
    "\n",
    "Wykorzystamy pakiet [MlXtend](https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/) oraz zawartą w nim funkcję `bias_variance_decomp`, która wykonuje następujące działania:\n",
    "\n",
    "```{prf:algorithm} Dekompozycja błędów\n",
    ":label: bias-variance-decomp\n",
    "\n",
    "**Wejścia** \n",
    "1. Zbiór danych treningowych $\\mathbf{X}$ wraz z wartościami docelowymi $\\mathbf{y}$, \n",
    "2. Zbiór danych testowych $\\mathbf{X}_{test}$ wraz z wartościami docelowymi $\\mathbf{y}_{test}$,\n",
    "3. Liczba iteracji próbkowania $k$,\n",
    "3. Funkcja kosztu $L$,\n",
    "3. Model $h$\n",
    "\n",
    "**Wyjście** Wyliczenie błędu obciążenia, wariancji i średniej oczekiwanej funkcji kosztu.\n",
    "\n",
    "1. Dla każdej iteracji $i$ od $1$ do $k$:\n",
    "    1. Wybierz losowo ze zwracaniem (metodą `bootstrap`) próbkę ze zbioru treningowego $\\mathbf{X}', \\mathbf{y}'$ z $\\mathbf{X}, \\mathbf{y}$.\n",
    "    2. Wytrenuj model $h$ na zbiorze $\\mathbf{X}', \\mathbf{y}'$.\n",
    "    3. Dokonaj predykcji na zbiorze testowym.\n",
    "    4. Zapisz różnice pomiędzy predykcją a wartością docelową dla każdej obserwacji.\n",
    "\n",
    "2. Za pomocą {ref}`bias-variance-derivation` oblicz błąd obciążenia, wariancję i błąd nieredukowalny.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy zachowanie kilku modeli:\n",
    "\n",
    "1. **Prosta regresja liniowa** - oczekujemy najmniejszej wariancji i dużego obciążenia,\n",
    "2. **Pojedyncze drzewo decyzyjne dla regresji** - oczekujemy średniej wariancji i średniego obciążenia.\n",
    "3. **Las losowy** - oczekujemy małego obciążenia i małej wariancji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3017a2dfcf4a6da784a51a9db44e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Expected Loss</th>\n",
       "      <th>Bias</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Training Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.316762</td>\n",
       "      <td>0.294988</td>\n",
       "      <td>0.021775</td>\n",
       "      <td>26.608934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.456935</td>\n",
       "      <td>0.294924</td>\n",
       "      <td>0.162011</td>\n",
       "      <td>6.042207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.556295</td>\n",
       "      <td>0.555354</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.391138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model  Expected Loss      Bias  Variance  Training Time\n",
       "2      Random Forest       0.316762  0.294988  0.021775      26.608934\n",
       "1      Decision Tree       0.456935  0.294924  0.162011       6.042207\n",
       "0  Linear Regression       0.556295  0.555354  0.000942       0.391138"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results = []\n",
    "\n",
    "models = [\n",
    "    ('Linear Regression', LinearRegression()),\n",
    "    ('Decision Tree', DecisionTreeRegressor(max_depth=10)),\n",
    "    ('Random Forest', RandomForestRegressor(n_estimators=25, max_depth=10, n_jobs=-1))\n",
    "]\n",
    "\n",
    "for name, model in tqdm(models):\n",
    "    train_start = time()\n",
    "    avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        model, X_train.values, y_train, X_test.values, y_test, \n",
    "        loss='mse',\n",
    "        num_rounds=100,\n",
    "        random_seed=123)\n",
    "    train_end = time()\n",
    "    trianing_time = train_end - train_start\n",
    "    model_results.append([name, avg_expected_loss, avg_bias, avg_var, trianing_time])\n",
    "\n",
    "model_results = pd.DataFrame(model_results, columns=['Model', 'Expected Loss', 'Bias', 'Variance', 'Training Time'])\n",
    "model_results.sort_values('Expected Loss', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że nasze obserwacje się potwierdziły:\n",
    "\n",
    "1. **Regresja liniowa** - ma największe obciążenie, ale najmniejszą, niemal zerową wariancję.\n",
    "2. **Drzewo decyzyjne** - ma średnie obciążenie i wariancję.\n",
    "3. **Las losowy** - ma najmniejsze obciążenie i wariancję.\n",
    "\n",
    "\n",
    "Oczywiście wszystko to jest funkcją złożoności modelu - spójrzmy na czas szkolenia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
